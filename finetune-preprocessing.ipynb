{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Training Finetune T5-small for texts classification","metadata":{}},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq, TrainerCallback\nimport evaluate\nimport torch\nimport numpy as np\nimport pandas as pd\n\n# Model\nmodel_name = \"google-t5/t5-small\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\nmodel.gradient_checkpointing_enable()\n\n# Hyperparams\nmax_input_len = 512\nmax_target_len = 128\n\n# Preprocess\ndef preprocess(example):\n    inputs = tokenizer(\n        \"clean: \" + example[\"input\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=max_input_len,\n    )\n    targets = tokenizer(\n        example[\"target\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=max_target_len,\n    )\n    inputs[\"labels\"] = targets[\"input_ids\"]\n    return inputs\n\ntokenized_ds = dataset.map(preprocess, batched=False)\nsplit = tokenized_ds.train_test_split(test_size=0.1)\ntrain_ds = split[\"train\"]\nval_ds = split[\"test\"]\n\n# Metric\nrouge = evaluate.load(\"rouge\")\n\ndef compute_metrics(eval_pred):\n    preds, labels = eval_pred\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    if preds.ndim == 3:\n        preds = np.argmax(preds, axis=-1)\n\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    decoded_preds = [pred.strip() for pred in decoded_preds]\n    decoded_labels = [label.strip() for label in decoded_labels]\n\n    return rouge.compute(predictions=decoded_preds, references=decoded_labels)\n\n# Clear GPU cache between train/eval\nclass CudaClearCallback(TrainerCallback):\n    def on_evaluate(self, args, state, control, **kwargs):\n        torch.cuda.empty_cache()\n\n# Training args\ntraining_args = TrainingArguments(\n    output_dir=\"./t5_clean_model\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=10,\n    warmup_ratio=0.2,\n    learning_rate=3e-5,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    logging_steps=20,\n    fp16=True,\n    report_to=\"none\",\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n    compute_metrics=compute_metrics,\n    callbacks=[CudaClearCallback()],\n)\n\n# Train\ntrainer.train()\n\n# Save\ntrainer.save_model(\"./t5_clean_final\")\ntokenizer.save_pretrained(\"./t5_clean_final\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference after training check texts preprocessing","metadata":{}},{"cell_type":"code","source":"# Load fine-tuned model and tokenizer\nmodel_dir = \"./t5_clean_final\"\ntokenizer = T5Tokenizer.from_pretrained(model_dir)\nmodel = T5ForConditionalGeneration.from_pretrained(model_dir).to(\"cuda\")\nmodel.eval()\n\n# Hyperparameters\nmax_input_len = 256\nmax_target_len = 128\n\n# Load new CSV data (must contain a column \"input\")\ndf = pd.read_csv(\"new_data.csv\")\n\n# Function to generate predictions for a batch of texts\ndef generate_clean_text(batch_texts):\n    # Tokenize input with prefix \"clean: \"\n    inputs = tokenizer(\n        [\"clean: \" + t for t in batch_texts],\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=max_input_len,\n    ).to(\"cuda\")\n\n    # Generate predictions (greedy decoding)\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_length=max_target_len,\n            num_beams=1,      # use greedy decoding (faster, less memory)\n            do_sample=False,\n        )\n\n    # Decode predictions into text\n    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    return [d.strip() for d in decoded]\n\n# Run inference in batches\nbatch_size = 16\nresults = []\nfor i in range(0, len(df), batch_size):\n    batch_texts = df[\"input\"].iloc[i:i+batch_size].tolist()\n    preds = generate_clean_text(batch_texts)\n    results.extend(preds)\n\n# Save results to new CSV\ndf[\"prediction\"] = results\ndf.to_csv(\"predictions.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}